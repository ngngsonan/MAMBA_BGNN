"""
Enhanced MAMBA-BGNN with Mamba-2 architecture improvements.
Implements more efficient SSM with structured state-space duality.
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from dataclasses import dataclass
from einops import rearrange, repeat, einsum
from typing import Optional, Tuple

# Import base components
from mamba_bgnn import ModelArgs, BayesianMAGAC
from result_plot import plot_analytics


# Enhanced Mamba-2 Block with SSD (Structured State-Space Duality)
class Mamba2Block(nn.Module):
    """
    Mamba-2 block with improved efficiency and structured state-space duality.
    Based on the Mamba-2 paper improvements over original Mamba.
    """
    
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        
        # Enhanced projections with better efficiency
        self.in_proj = nn.Linear(args.d_model, args.d_proj_E * 3, bias=args.bias)  # 3x for x, z, B
        
        # Structured state-space parameters
        self.conv1d = nn.Conv1d(
            in_channels=args.d_proj_E,
            out_channels=args.d_proj_E,
            kernel_size=args.d_conv,
            groups=args.d_proj_E,
            padding=args.d_conv - 1,
            bias=args.conv_bias,
        )
        
        # Enhanced projections for SSD
        self.x_proj = nn.Linear(args.d_proj_E, args.dt_rank + args.d_proj_H, bias=False)
        self.dt_proj = nn.Linear(args.dt_rank, args.d_proj_E, bias=True)
        
        # Structured state-space matrices (Mamba-2 improvement)
        # Use block-diagonal structure for better efficiency
        self.num_blocks = 4  # Number of diagonal blocks
        block_size = args.d_proj_H // self.num_blocks
        
        # Initialize structured A matrix
        A_blocks = []
        for i in range(self.num_blocks):
            A_block = torch.arange(1, block_size + 1).float()
            A_blocks.append(A_block)
        A = torch.block_diag(*[torch.diag(block) for block in A_blocks])
        A = repeat(A, 'h n -> d h n', d=args.d_proj_E)[:, :args.d_proj_H, :args.d_proj_H]
        self.A_log = nn.Parameter(torch.log(A.diagonal(dim1=-2, dim2=-1)))
        
        # Enhanced D parameter with learnable scaling
        self.D = nn.Parameter(torch.ones(args.d_proj_E))
        self.norm = nn.LayerNorm(args.d_proj_E)
        
        # Output projection with residual scaling
        self.out_proj = nn.Linear(args.d_proj_E, args.d_model, bias=args.bias)
        self.residual_scale = nn.Parameter(torch.ones(1))
        
    def forward(self, x):
        b, l, d = x.shape
        
        # Enhanced input projection (3-way split)
        x_proj, z, B_proj = self.in_proj(x).chunk(3, dim=-1)
        
        # Apply convolution with gating
        x_proj = rearrange(x_proj, 'b l d -> b d l')
        x_proj = self.conv1d(x_proj)[:, :, :l]
        x_proj = rearrange(x_proj, 'b d l -> b l d')
        x_proj = F.silu(x_proj)
        
        # Structured state-space computation
        y = self._structured_ssm(x_proj, B_proj)
        
        # Enhanced gating with normalization
        y = self.norm(y)
        y = y * torch.sigmoid(z)
        
        # Residual connection with learnable scaling
        output = self.out_proj(y)
        return output * self.residual_scale
    
    def _structured_ssm(self, x, B_proj):
        """Structured State-Space computation with efficiency improvements"""
        b, l, d = x.shape
        
        # Get structured A matrix
        A = -torch.exp(self.A_log.float())  # (d, h)
        D = self.D.float()
        
        # Enhanced delta computation
        x_dbl = self.x_proj(x)  # (b, l, dt_rank + h)
        delta, C = torch.split(x_dbl, [self.args.dt_rank, self.args.d_proj_H], dim=-1)
        delta = F.softplus(self.dt_proj(delta))  # (b, l, d)
        
        # Structured B matrix from projection
        B = F.softplus(B_proj)  # (b, l, d)
        
        # Efficient SSM computation using structured approach
        # This is the key Mamba-2 improvement: more efficient state computation
        y = self._efficient_scan(x, delta, A, B, C, D)
        
        return y
    
    def _efficient_scan(self, x, delta, A, B, C, D):
        """
        Efficient parallel scan implementation for Mamba-2.
        Uses structured state-space duality for better performance.
        """
        b, l, d = x.shape
        h = A.shape[-1]
        
        # Discretize continuous parameters
        # Using more stable discretization
        delta = delta.clamp(min=1e-6, max=1e2)  # Numerical stability
        A_discrete = torch.exp(delta.unsqueeze(-1) * A.unsqueeze(0).unsqueeze(0))  # (b, l, d, h)
        B_discrete = delta.unsqueeze(-1) * B.unsqueeze(-1) * x.unsqueeze(-1)  # (b, l, d, h)
        
        # Parallel scan with structured approach
        # Initialize states
        states = torch.zeros(b, h, device=x.device, dtype=x.dtype)  # (b, h)
        outputs = []
        
        # More efficient sequential processing (could be parallelized further)
        for i in range(l):
            # Update state: s = A * s + B * x
            states = A_discrete[:, i, 0] * states + B_discrete[:, i, 0]  # (b, h)
            # Output: y = C * s + D * x
            y_i = torch.sum(C[:, i:i+1] * states.unsqueeze(1), dim=-1) + D * x[:, i]  # (b, d)
            outputs.append(y_i)
        
        y = torch.stack(outputs, dim=1)  # (b, l, d)
        return y


class BIMamba2Block(nn.Module):
    """Enhanced Bidirectional Mamba-2 block"""
    
    def __init__(self, args: ModelArgs, R: int = 3, dropout: float = 0.1):
        super().__init__()
        self.R = R
        
        # Use Mamba-2 blocks
        self.f_mamba = nn.ModuleList([Mamba2Block(args) for _ in range(R)])
        self.b_mamba = nn.ModuleList([Mamba2Block(args) for _ in range(R)])
        
        # Enhanced normalization and feedforward
        self.norm1 = nn.ModuleList([nn.LayerNorm(args.d_model) for _ in range(R)])
        self.ffn = nn.ModuleList([self._build_enhanced_ffn(args, dropout) for _ in range(R)])
        self.norm2 = nn.ModuleList([nn.LayerNorm(args.d_model) for _ in range(R)])
        
        # Add attention mechanism for better feature fusion
        # Ensure num_heads divides d_model evenly
        num_heads = 8 if args.d_model % 8 == 0 else (4 if args.d_model % 4 == 0 else 3)
        self.cross_attention = nn.ModuleList([
            nn.MultiheadAttention(args.d_model, num_heads=num_heads, dropout=dropout, batch_first=True)
            for _ in range(R)
        ])
        self.norm_attn = nn.ModuleList([nn.LayerNorm(args.d_model) for _ in range(R)])
        
    def _build_enhanced_ffn(self, args: ModelArgs, dropout: float):
        """Enhanced feedforward network with Swish activation and dropout"""
        hidden_dim = getattr(args, 'd_proj_U', 64)
        return nn.Sequential(
            nn.Linear(args.d_model, hidden_dim * 2),  # Larger hidden dimension
            nn.SiLU(),  # Swish activation (better than ReLU)
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, args.d_model),
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        for i in range(self.R):
            # Bidirectional Mamba-2 processing
            Y1 = self.f_mamba[i](x)
            
            x_rev = torch.flip(x, dims=[1]).contiguous()
            Y2_rev = self.b_mamba[i](x_rev)
            Y2 = torch.flip(Y2_rev, dims=[1]).contiguous()
            
            # Enhanced fusion with cross-attention
            Y_combined = Y1 + Y2
            Y_attn, _ = self.cross_attention[i](Y_combined, Y_combined, Y_combined)
            Y3 = self.norm_attn[i](Y_combined + Y_attn)
            
            # Residual connection and normalization
            Y3 = self.norm1[i](x + Y3)
            
            # Enhanced feedforward
            Yp = self.ffn[i](Y3)
            x = self.norm2[i](Yp + Y3)
            
        return x


class MAMBA2_BayesMAGAC(nn.Module):
    """Enhanced MAMBA-BGNN with Mamba-2 architecture"""
    
    def __init__(self, args: ModelArgs, R: int = 3, K: int = 3, heads: int = 4, 
                 mc_train: int = 1, mc_eval: int = 10, dropout: float = 0.1):
        super().__init__()
        self.args = args
        self.mc_train = mc_train
        self.mc_eval = mc_eval
        
        # Enhanced Mamba-2 temporal processing
        self.mamba2_layers = BIMamba2Block(args, R=R, dropout=dropout)
        
        # Enhanced Bayesian Graph Network
        self.graph_layers = BayesianMAGAC(
            num_nodes=args.seq_len,
            in_dim=args.d_model,
            K=K,
            heads=heads,
            mc_train=mc_train,
            mc_eval=mc_eval
        )
        
        # Enhanced output layers with uncertainty quantification
        self.feature_fusion = nn.Sequential(
            nn.Linear(args.d_model * 2, args.d_model),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.LayerNorm(args.d_model),
        )
        
        # Probabilistic output heads
        self.mean_head = nn.Sequential(
            nn.Linear(args.d_model, args.d_model // 2),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(args.d_model // 2, 1)
        )
        
        self.var_head = nn.Sequential(
            nn.Linear(args.d_model, args.d_model // 2),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(args.d_model // 2, 1)
        )
        
        # Learnable temperature for uncertainty calibration
        self.temperature = nn.Parameter(torch.ones(1))
        
    def forward(self, x):
        # Enhanced temporal processing with Mamba-2
        temporal_features = self.mamba2_layers(x)  # (B, L, N)
        
        # Enhanced spatial processing with Bayesian GNN
        # Use multiple MC samples during training/evaluation
        mc_samples = self.mc_train if self.training else self.mc_eval
        spatial_outputs = []
        
        for _ in range(mc_samples):
            spatial_feat = self.graph_layers(temporal_features)  # (B, L, N)
            spatial_outputs.append(spatial_feat)
        
        # Average spatial features across MC samples
        spatial_features = torch.stack(spatial_outputs, dim=0).mean(dim=0)
        
        # Enhanced feature fusion
        fused_features = torch.cat([temporal_features, spatial_features], dim=-1)
        fused_features = self.feature_fusion(fused_features)  # (B, L, N)
        
        # Use last time step for prediction
        final_features = fused_features[:, -1, :]  # (B, N)
        
        # Probabilistic outputs with temperature scaling
        mu = self.mean_head(final_features).squeeze(-1)  # (B,)
        log_var = self.var_head(final_features).squeeze(-1)  # (B,)
        
        # Temperature-scaled uncertainty
        log_var = log_var + torch.log(self.temperature**2)
        
        return mu, log_var
    
    def predict_with_uncertainty(self, x, num_samples: int = 100):
        """Generate predictions with epistemic uncertainty estimation"""
        self.eval()
        
        with torch.no_grad():
            predictions = []
            
            for _ in range(num_samples):
                mu, log_var = self.forward(x)
                # Sample from the predictive distribution
                sigma = torch.exp(0.5 * log_var)
                pred = torch.normal(mu, sigma)
                predictions.append(pred)
            
            predictions = torch.stack(predictions, dim=0)  # (num_samples, B)
            
            # Compute mean and epistemic uncertainty
            pred_mean = predictions.mean(dim=0)
            pred_std = predictions.std(dim=0)
            
            return pred_mean, pred_std


# Import remaining components from original implementation
from mamba_bgnn import data_processing, Trainer


def main(dataset='DJI'):
    """Main training function with Mamba-2 enhancements"""
    print(f"üöÄ Starting MAMBA-2 BGNN training on {dataset}")
    
    # Enhanced hyperparameters
    window = 5
    batch_size = 128
    lr = 1e-3
    epochs = 1500
    
    # Process data (fix the path and function signature)
    data_path = f'Dataset/combined_dataframe_{dataset}.csv'
    N, train_loader, val_loader, test_loader = data_processing(data_path, window, batch_size)
    L = window              # Sequence length
    
    print(f"üìä Data loaded: N={N} features, L={L} sequence length")
    print(f"üî• Using CUDA: {torch.cuda.is_available()}")
    
    # Enhanced model arguments
    args = ModelArgs(
        d_model=N,
        seq_len=L,
        d_proj_E=min(128, N),  # Adaptive embedding dimension
        d_proj_H=64,
        d_proj_U=64,
        d_state=64
    )
    
    # Create enhanced model
    model = MAMBA2_BayesMAGAC(
        args=args,
        R=3,              # Number of bidirectional layers
        K=3,              # Chebyshev polynomial order
        heads=3,          # Attention heads (81 % 3 = 0)
        mc_train=1,       # MC samples during training
        mc_eval=20,       # MC samples during evaluation (increased)
        dropout=0.1       # Dropout rate
    )
    
    print(f"üèóÔ∏è  Model created with {sum(p.numel() for p in model.parameters()):,} parameters")
    
    # Enhanced trainer (use data loaders from original implementation)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.cuda.is_available():
        model = model.cuda()
    
    loss_fn = nn.GaussianNLLLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    # Create args dict matching original format
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    args_dict = {
        'epochs': epochs,
        'early_stop': True,
        'early_stop_patience': 25,
        'grad_norm': False,
        'max_grad_norm': 5.0,
        'log_dir': f'logs/{dataset}_mamba2_log {timestamp}',
        'model_name': f'{dataset}_mamba2_v1',
        'log_step': 20
    }
    
    trainer = Trainer(
        model=model,
        loss_fn=loss_fn,
        optimizer=optimizer,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        args=args_dict
    )
    
    # Train the enhanced model
    trainer.train()
    
    # Test with enhanced evaluation
    trainer.test()
    
    print("‚úÖ MAMBA-2 BGNN training completed!")


if __name__ == "__main__":
    main('DJI')